{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b834eae9-e4f9-44ec-bd32-acbae31bb977",
   "metadata": {},
   "source": [
    "# Multimodal Model for Dance & Text    \n",
    "\n",
    "### 1. Overview  \n",
    "\n",
    "This project aims to train a **multimodal model** that learns a **shared embedding space** for dance sequences and natural language descriptions. The model leverages **contrastive learning** to align dance phrases with their corresponding text descriptions.  \n",
    "\n",
    "### Capabilities:  \n",
    "- **Generating dance sequences** from text descriptions.  \n",
    "- **Generating text descriptions** from dance sequences.  \n",
    "\n",
    "### Project Workflow:  \n",
    "1. **Data Preparation**: Preprocess motion capture data and generate text labels.  \n",
    "2. **Model Design**: Define the dance encoder, text encoder, and contrastive loss function.  \n",
    "3. **Training**: Train the model using contrastive learning to align dance and text embeddings.  \n",
    "4. **Evaluation**: Test the model by generating dance sequences from text and vice versa.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6912-2af2-453e-b5c3-1f6c12fe72cd",
   "metadata": {},
   "source": [
    "# 2. Data Preparation  \n",
    "\n",
    "## 2.1. Motion Capture Data  \n",
    "\n",
    "The motion capture data is loaded and preprocessed to extract **fixed-length dance phrases**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fc415df-2413-4340-883a-23c9655861e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load motion capture data\n",
    "data = np.load('mariel_betternot_and_retrograde.npy')  # Shape: (# joints, # timesteps, # dimensions)\n",
    "data = np.transpose(data, (1, 0, 2))  # Reshape to (# timesteps, # joints, # dimensions)\n",
    "\n",
    "# Extract fixed-length dance phrases\n",
    "phrase_length = 30  # Example: 30 timesteps\n",
    "dance_phrases = [data[i:i+phrase_length] for i in range(0, len(data) - phrase_length, phrase_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcfa9e-80d1-41a0-a8bb-a1f157e73eb4",
   "metadata": {},
   "source": [
    "## 2.2. Generate Natural Language Labels  \n",
    "\n",
    "Since there are no ground truth labels, we generate them in a **semi-supervised** way:  \n",
    "\n",
    "- **Manual Labeling**: Label **1%** of the dance phrases with descriptive text.  \n",
    "- **Automatic Labeling**: Use **KMeans clustering** to group similar dance phrases and assign labels.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fdfcb24-cb84-471a-8fd6-4fb18d3dd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Flatten dance phrases for clustering\n",
    "n_phrases, n_timesteps, n_joints, n_dims = len(dance_phrases), phrase_length, data.shape[1], data.shape[2]\n",
    "flattened_phrases = np.array(dance_phrases).reshape(n_phrases, -1)  # Shape: (# phrases, # timesteps * # joints * # dimensions)\n",
    "\n",
    "# Perform KMeans clustering\n",
    "n_clusters = 10  # Number of clusters (adjust based on data)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(flattened_phrases)\n",
    "\n",
    "# Assign labels to dance phrases\n",
    "manual_labels = {0: \"kick\", 1: \"spin\", 2: \"slow\", 3: \"smooth\", 4: \"sharp\", 5: \"jump\", 6: \"turn\", 7: \"wave\", 8: \"pause\", 9: \"run\"}\n",
    "dance_labels = [manual_labels[label] for label in cluster_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609d88f-5b1a-4c23-b42a-40dced10a3ca",
   "metadata": {},
   "source": [
    "# 3. Model Design  \n",
    "\n",
    "## 3.1. Dance Encoder  \n",
    "\n",
    "The **dance encoder** is an **LSTM-based neural network** that encodes dance phrases into embeddings.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "751fee24-8f78-4667-be31-dd9ea6351e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DanceEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DanceEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50c964-4e5b-4205-b7eb-5a8a467a442f",
   "metadata": {},
   "source": [
    "## 3.2. Text Encoder  \n",
    "\n",
    "The **text encoder** uses a **pretrained BERT model** to encode text descriptions into embeddings.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f116739-2ccf-4c3a-ab44-3edd8ab53255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.bert(x)\n",
    "        return self.fc(outputs.last_hidden_state[:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9bee8-c159-4795-99ab-e03e7ac8ea35",
   "metadata": {},
   "source": [
    "## 3.3. Contrastive Loss  \n",
    "\n",
    "The **contrastive loss** ensures that **similar dance-text pairs** are close in the embedding space, while **dissimilar pairs** are far apart.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adba495a-3e8b-4d42-b303-20108ee2b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, dance_emb, text_emb, labels):\n",
    "        distance = torch.norm(dance_emb - text_emb, dim=1)\n",
    "        loss = (labels * distance.pow(2) + (1 - labels) * torch.relu(self.margin - distance).pow(2)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527873a1-db46-462f-9bca-0f98326f454b",
   "metadata": {},
   "source": [
    "# 4. Training  \n",
    "\n",
    "## 4.1. Data Augmentation  \n",
    "\n",
    "- Apply transformations to **dance phrases** (e.g., noise, rotation).  \n",
    "- Use **text augmentation** (e.g., synonym replacement) for text descriptions.  \n",
    "\n",
    "## 4.2. Training Loop  \n",
    "\n",
    "Train the model using the **contrastive loss**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "152b3d14-7597-4770-9d5d-34d12ad2fa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d673cf965aa4edda4af3d199dc9936c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281a1710533c407fb7de0fc3a39c5379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1e5bdd574c4f2db3e652021b6502ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: -4.667327880859375\n",
      "Epoch 2, Loss: -4.998797416687012\n",
      "Epoch 3, Loss: -5.015745162963867\n",
      "Epoch 4, Loss: -5.394674777984619\n",
      "Epoch 5, Loss: -5.471688747406006\n",
      "Epoch 6, Loss: -5.535956859588623\n",
      "Epoch 7, Loss: -5.922285079956055\n",
      "Epoch 8, Loss: -6.0956830978393555\n",
      "Epoch 9, Loss: -6.271459102630615\n",
      "Epoch 10, Loss: -6.383434295654297\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize models\n",
    "dance_encoder = DanceEncoder(input_dim=n_joints * n_dims, hidden_dim=128, output_dim=64)\n",
    "text_encoder = TextEncoder(output_dim=64)\n",
    "optimizer = optim.Adam(list(dance_encoder.parameters()) + list(text_encoder.parameters()), lr=1e-4)\n",
    "criterion = ContrastiveLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "    for dance_phrase, text, label in zip(dance_phrases, dance_labels, cluster_labels):\n",
    "        # Reshape dance_phrase to (sequence_length, input_size)\n",
    "        dance_phrase_reshaped = dance_phrase.reshape(phrase_length, n_joints * n_dims)\n",
    "        \n",
    "        # Convert dance phrase to tensor\n",
    "        dance_phrase_tensor = torch.tensor(dance_phrase_reshaped, dtype=torch.float32)\n",
    "        \n",
    "        # Tokenize text input\n",
    "        text_tensor = tokenizer(text, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        \n",
    "        # Forward pass\n",
    "        dance_emb = dance_encoder(dance_phrase_tensor)\n",
    "        text_emb = text_encoder(text_tensor)  # Pass tokenized text to the text encoder\n",
    "        loss = criterion(dance_emb, text_emb, label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26058c-a9f0-42bb-ac3b-e16f427fedce",
   "metadata": {},
   "source": [
    "# 5. Evaluation  \n",
    "\n",
    "## 5.1. Generating Dance from Text  \n",
    "\n",
    "Use the trained **text encoder** to generate an embedding for a text description and find the **nearest dance phrase embedding**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d641bbd-cfe3-42ea-97f6-917c108143b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dance_from_text(text):\n",
    "    # Generate text embedding\n",
    "    text_emb = text_encoder(text)\n",
    "    \n",
    "    # Compute distances to all dance embeddings\n",
    "    distances = []\n",
    "    for dance in dance_phrases:\n",
    "        # Reshape dance phrase to (sequence_length, input_size)\n",
    "        dance_reshaped = dance.reshape(phrase_length, n_joints * n_dims)  # Reshape to (30, 165)\n",
    "        # Convert dance phrase to tensor\n",
    "        dance_tensor = torch.tensor(dance_reshaped, dtype=torch.float32)\n",
    "        # Generate dance embedding\n",
    "        dance_emb = dance_encoder(dance_tensor)\n",
    "        # Compute distance between text and dance embeddings\n",
    "        distance = torch.norm(text_emb - dance_emb)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    # Find the closest dance sequence\n",
    "    closest_dance = dance_phrases[torch.argmin(torch.tensor(distances))]\n",
    "    \n",
    "    return closest_dance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53582e7-e249-45ce-9a67-63eddd676f1f",
   "metadata": {},
   "source": [
    "## 5.2. Generating Text from Dance  \n",
    "\n",
    "Use the trained **dance encoder** to generate an embedding for a dance phrase and find the **nearest text embedding**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51bcb61a-571f-485d-81fa-16d7adb71744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_dance(dance):\n",
    "    # Reshape the dance sequence to (sequence_length, input_size)\n",
    "    dance_reshaped = dance.reshape(phrase_length, n_joints * n_dims)  # Reshape to (30, 165)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    dance_tensor = torch.tensor(dance_reshaped, dtype=torch.float32)\n",
    "    \n",
    "    # Generate dance embedding\n",
    "    dance_emb = dance_encoder(dance_tensor)\n",
    "    \n",
    "    # Compute distances to all text embeddings\n",
    "    distances = []\n",
    "    for text in dance_labels:\n",
    "        # Tokenize the text\n",
    "        text_tensor = tokenizer(text, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # Generate text embedding\n",
    "        text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da70cdba-5f01-4138-bccf-c83a2643e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dance sequence shape: (30, 55, 3)\n"
     ]
    }
   ],
   "source": [
    "# Example text input\n",
    "text_description = \"slow spin with a kick\"\n",
    "\n",
    "# Tokenize the text input (same as during training)\n",
    "text_tensor = tokenizer(text_description, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "\n",
    "# Generate dance sequence from text\n",
    "generated_dance = generate_dance_from_text(text_tensor)\n",
    "print(f\"Generated dance sequence shape: {generated_dance.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c26100af-3f62-4143-b754-49b61b675977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-2.06312513 -5.39422274 -1.61872065]\n",
      "  [-2.0088675  -5.36202002 -1.3472805 ]\n",
      "  [-2.14767075 -5.47529221 -1.29353142]\n",
      "  ...\n",
      "  [-2.15119791 -5.49386215 -1.08909988]\n",
      "  [-2.08163595 -5.45382738 -0.83872068]\n",
      "  [-1.97276092 -5.34514713 -1.16818988]]\n",
      "\n",
      " [[-2.09504938 -5.44246817 -1.6213491 ]\n",
      "  [-2.04460382 -5.40191174 -1.35064662]\n",
      "  [-2.1795938  -5.51978683 -1.29737091]\n",
      "  ...\n",
      "  [-2.18267584 -5.54136705 -1.09420991]\n",
      "  [-2.11699033 -5.48914766 -0.83960086]\n",
      "  [-2.00827456 -5.38622856 -1.17073238]]\n",
      "\n",
      " [[-2.12278891 -5.4755578  -1.62462592]\n",
      "  [-2.07807064 -5.43500948 -1.35772872]\n",
      "  [-2.20409489 -5.56046677 -1.30337548]\n",
      "  ...\n",
      "  [-2.20648813 -5.58359861 -1.10193825]\n",
      "  [-2.15006471 -5.52228975 -0.84372193]\n",
      "  [-2.0424273  -5.41833305 -1.17673659]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.15840364 -5.54407263 -1.63721228]\n",
      "  [-2.11121583 -5.47131586 -1.36742949]\n",
      "  [-2.2308507  -5.60166311 -1.31303537]\n",
      "  ...\n",
      "  [-2.23591852 -5.62535191 -1.11280727]\n",
      "  [-2.18688416 -5.55673838 -0.8511765 ]\n",
      "  [-2.0797677  -5.45239544 -1.18521094]]\n",
      "\n",
      " [[-2.15840364 -5.54407263 -1.63721228]\n",
      "  [-2.11121583 -5.47131586 -1.36742949]\n",
      "  [-2.2308507  -5.60166311 -1.31303537]\n",
      "  ...\n",
      "  [-2.23591852 -5.62535191 -1.11280727]\n",
      "  [-2.18688416 -5.55673838 -0.8511765 ]\n",
      "  [-2.0797677  -5.45239544 -1.18521094]]\n",
      "\n",
      " [[-2.15840364 -5.54407263 -1.63721228]\n",
      "  [-2.11121583 -5.47131586 -1.36742949]\n",
      "  [-2.2308507  -5.60166311 -1.31303537]\n",
      "  ...\n",
      "  [-2.23591852 -5.62535191 -1.11280727]\n",
      "  [-2.18688416 -5.55673838 -0.8511765 ]\n",
      "  [-2.0797677  -5.45239544 -1.18521094]]]\n"
     ]
    }
   ],
   "source": [
    "print(generated_dance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
